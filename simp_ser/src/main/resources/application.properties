#服务端口
#server.address=127.0.0.1
server.address.mapper=127.0.0.1
server.port=8081
server.tomcat.uri-encoding=UTF-8
server.tomcat.min-spare-threads: 50
server.tomcat.max-threads 2000
server.connection-timeout: 5000

#activiti
spring.activiti.check-process-definitions=true
spring.activiti.process-definition-location-prefix: classpath:/processes/
spring.activiti.async-executor-activate=true
spring.activiti.database-schema-update: true
spring.activiti.history-level: full
spring.activiti.db-history-used: true


#cache
#spring.cache.type=caffeine
spring.cache.type=redis
spring.cache.cache-names=menuCache,pagePerCache,perCache,userCache,dictCache,roleCache,smCache,mesGroupsCache,starCache
spring.cache.caffeine.spec=maximumSize=50000, expireAfterWrite=300s

#redis
spring.redis.host= 192.168.120.31
spring.redis.port= 6379
spring.redis.password= Redis123.
spring.redis.database= 14
spring.redis.timeout= 2000

# 连接池中的最小空闲连接
spring.redis.lettuce.pool.min-idle=0  
# 连接池中的最大空闲连接
spring.redis.lettuce.pool.max-idle= 50
# 连接池最大阻塞等待时间（使用负值表示没有限制）
spring.redis.lettuce.pool.max-wait= -1s
# 连接池最大连接数（使用负值表示没有限制）
spring.redis.lettuce.pool.max-active= -1

#自动化切面
spring.aop.auto=true

#kafka
spring.kafka.bootstrap-servers: 192.168.120.31:9092
spring.kafka.producer.retries: 0
# 每次批量发送消息的数量
spring.kafka.producer.batch-size: 16384
# 缓存容量
spring.kafka.producer.buffer-memory: 33554432
# 指定消息key和消息体的编解码方式
spring.kafka.producer.key-serializer: org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer: org.apache.kafka.common.serialization.StringSerializer
# 指定默认消费者group id
spring.kafka.consumer.group-id:consumer-tutorial
spring.kafka.consumer.auto-commit-interval: 100
spring.kafka.consumer.auto-offset-reset: earliest
spring.kafka.consumer.enable-auto-commit: true
# 指定消息key和消息体的编解码方式
spring.kafka.consumer.key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
# 指定listener 容器中的线程数，用于提高并发量
spring.kafka.listener.concurrency: 4
spring.kafka.max.poll.records:10

#时区指定，时间格式化
spring.jackson.time-zone=GMT+8
spring.jackson.date-format=yyyy-MM-dd HH:mm:ss

#mango db
spring.data.mongodb.uri=mongodb://smmprw:Smmprw123.@192.168.120.31:27017/smmp

#数据库连接
spring.datasource.type=com.alibaba.druid.pool.DruidDataSource
spring.datasource.druid.driver-class-name=com.mysql.cj.jdbc.Driver
spring.datasource.url=jdbc:mysql://192.168.120.31:3306/smmp_dev?nullCatalogMeansCurrent=true&useUnicode=true&characterEncoding=UTF-8&allowMultiQueries=true&serverTimezone=Hongkong&useSSL=false
spring.datasource.username=root
spring.datasource.password=Root123.
#spring.datasource.url=jdbc:mysql://192.168.120.31:3306/smmp_dev?nullCatalogMeansCurrent=true&useUnicode=true&characterEncoding=UTF-8&allowMultiQueries=true&serverTimezone=Hongkong&useSSL=false
#spring.datasource.username=root
#spring.datasource.password=Root123.

#连接池
#配置初始化大小/最小/最大
spring.datasource.druid.initial-size=50
spring.datasource.druid.min-idle=20
spring.datasource.druid.max-active=1000
#获取连接等待超时时间
spring.datasource.druid.max-wait=60000
#间隔多久进行一次检测，检测需要关闭的空闲连接
spring.datasource.druid.time-between-eviction-runs-millis=60000
#一个连接在池中最小生存的时间
spring.datasource.druid.min-evictable-idle-time-millis=30000
spring.datasource.druid.validation-query=SELECT 1 FROM DUAL
spring.datasource.druid.validation-query-timeout=6000
spring.datasource.druid.test-while-idle=true
spring.datasource.druid.test-on-borrow=false
spring.datasource.druid.test-on-return=false
#打开PSCache，并指定每个连接上PSCache的大小。oracle设为true，mysql设为false。分库分表较多推荐设置为false¸ºfalse
spring.datasource.druid.pool-prepared-statements=false
spring.datasource.druid.max-pool-prepared-statement-per-connection-size=50
#打开PSCache，并指定每个连接上PSCache的大小。oracle设为true，mysql设为false。分库分表较多推荐设置为false
spring.datasource.druid.filters=stat

#mybaits
mybatis.mapper-locations=classpath:mapping/**/*.xml
mybatis.type-aliases-package=com.mpri.aio.base.model
mybatis.configuration.logImpl=org.apache.ibatis.logging.stdout.StdOutImpl 
mybatis.configuration.map-underscore-to-camel-case=true
mybatis.configuration.default-fetch-size=200
mybatis.configuration.default-statement-timeout=60

#pagehelper分页配置 
pagehelper.helperDialect=mysql
pagehelper.reasonable=true
pagehelper.supportMethodsArguments=true
pagehelper.params=count\=countSql    
#debug=true

logging.pattern.console="%-5level - %msg%n"
logging.level.com.onepro.custom.mapper=Error
logging.level.tk.mybatis.springboot.mapper=trace
#kafka日志
#logging.level.org.apache.kafka=Error
#activiti 日志
#logging.level.org.activiti.engine.impl.persistence.entity: trace


#本地文件资源对外暴露的访问路径
file.staticAccessPath=/upload/file/**
#文件上传目录（注意Linux和Windows上的目录结构不同）
#file.uploadFolder=/opt/project/smmp/resource/
file.uploadFolder=F://uploadFiles/

#上传文件大小
spring.servlet.multipart.max-file-size=16MB
spring.servlet.multipart.max-request-size=16MB



#备份
#json文件相关配置
json.oper.filename=E://opt//backup.json
#备份文件基	础路径
back.backpath=E://opt//
back.maxsize=15
#back.backpath=/opt/backup/


#mysqldump的执行路径
#back.mysql.mdpath=/usr/bin/mysqldump
#mysql的执行路径
#back.mysql.mypath=/usr/bin/mysql
#备份库名
#back.mysql.backbase=smmp_dev

back.mysql.mdpath=C://Program Files//MySQL//MySQL Server 5.7//bin//mysqldump
back.mysql.mypath=C://Program Files//MySQL//MySQL Server 5.7//bin//mysql
back.mysql.backbase=smmp_dev

back.mongo.dbname=smmp
back.mongo.dbuser=smmprw
back.mongo.dbpass=Smmprw123.
back.mongo.dburl=192.168.120.31:27017
back.mongo.mdpath=C://Program Files//MongoDB//Server//4.0//bin//mongodump
back.mongo.mspath=C://Program Files//MongoDB//Server//4.0//bin//mongorestore
#back.mongo.mdpath=mongodump
#back.mongo.mspath=mongorestore


##########clientlicense parameters 校验证书参数 ###########
clientlicense.PUBLICALIAS=publickey
#STOREPWD(该密码是在使用keytool生成密钥对时设置的密钥库的访问密码)
clientlicense.STOREPWD=Admin123.
#SUBJECT
clientlicense.SUBJECT=license
#shoolcode
clientlicense.schoolCode=20190925
clientlicense.schoolName=博达学院

#license open
license.openflag=false

